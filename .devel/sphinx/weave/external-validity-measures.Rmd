(sec:external-validity-measures)=
# External Cluster Validity Measures

In this section we review the external cluster validity scores that are implemented
in the [*genieclust*](https://genieclust.gagolewski.com/) package for Python
and R {cite}`genieclust` and discussed in detail in {cite}`aaa`
(this section contains excerpts therefrom).



Let $\mathbf{y}$ be a {ref}`label <sec:true-vs-predicted>`
vector representing {ref}`one <sec:many-partitions>` of the reference
$k$-partitions $\{X_1,\dots,X_k\}$ of a benchmark dataset[^footnoise] $X$,
where $y_i\in\{1,\dots,k\}$ gives the true cluster number (ID) of the $i$-th object.

Furthermore, let $\hat{\mathbf{y}}$ be a label vector
encoding another partition, $\{\hat{X}_1,\dots,\hat{X}_k\}$,
which we would like to *relate* to the reference one, $\mathbf{y}$.
In our context, we assume that $\hat{\mathbf{y}}$ has been determined by some
clustering algorithm.

**External cluster validity measures** are functions
of the form $I(\mathbf{y}, \hat{\mathbf{y}})$ such that
the more *similar* the partitions, the higher the score.
They are normalised so that identical partitions give the highest similarity
score which is equal to 1.
Some are adjusted for chance, yielding approximately 0 for random
partitionings.

Oftentimes, **partition similarity scores** (e.g.,
the adjusted Rand index or the normalised mutual information score)
are used as $I$s. They are symmetric in the sense that
$I(\mathbf{y}, \hat{\mathbf{y}})= I(\hat{\mathbf{y}}, \mathbf{y})$.
However, as argued in {cite}`aaa`, in our context we do not need this
property because the reference label vector $\mathbf{y}$ is considered
fixed. The adjusted asymmetric accuracy is an example of such a non-symmetric
measure.

[^footnoise]: We assume that any potential
    {ref}`noise points <sec:noise-points>` in $X$ have been removed
    before the data analysis.


## Confusion Matrix

Also, let $\mathbf{C}$ be corresponding *confusion* (matching)
${k\times k}$-matrix, where $c_{i,j}=\#\{ u: y_u=i\text{ and }\hat{y}_u=j \}$
denotes the number of points in the true cluster $X_i$ and
the predicted cluster $\hat{X}_j$.


$$
\begin{array}{|c||cccc|}
\hline
\mathbf{c}_{1,\cdot}   & c_{1,1}   & c_{1,2}  & \cdots & c_{1,k} \\
\mathbf{c}_{2,\cdot}   & c_{2,1}   & c_{2,2}  & \cdots & c_{2,k} \\
\mathbf{c}_{3,\cdot}   & c_{3,1}   & c_{3,2}  & \cdots & c_{3,k} \\
\vdots                 &  \vdots   & \vdots   & \ddots & \vdots  \\
\mathbf{c}_{k,\cdot}   & c_{k,1}   & c_{k,2}  & \cdots & c_{k,k} \\
\hline\hline
n & \mathbf{c}_{\cdot,1} & \mathbf{c}_{\cdot,2} & \cdots & \mathbf{c}_{\cdot,k} \\
\hline
\end{array}
$$


It of course holds $\sum_{i=1}^k \sum_{j=1}^k c_{i,j} = n$.
Moreover, let
$\mathbf{c}_{i,\cdot} = \sum_{j=1}^k c_{i,j} = \#\{ u: y_u=i \}$
denote the number of elements in the reference cluster $X_i$
and
$\mathbf{c}_{\cdot,j} = \sum_{i=1}^k c_{i,j} = \#\{ u: \hat{y}_u=j \}$
be the number of objects in the predicted cluster $\hat{X}_j$.

All the measures reviewed here are expressed solely by means of operations
on confusion matrices. Therefore, we will be using the notation
$I(\mathbf{y}, \hat{\mathbf{y}})$ and $I(\mathbf{C})$
interchangeably, implicitly assuming that this is
clear from the context that $\mathbf{C}$ (and $k$ and $n$)
are obtained by studying the corresponding pairs of elements in the
two label vectors.





## An Illustration

As an illustration, we consider the two-dimensional
[`wut/x2`](https://github.com/gagolews/clustering-data-v1) dataset.

```{python}
import numpy as np
import pandas as pd
dataset = "https://github.com/gagolews/clustering-data-v1/raw/master/wut/x2"
X = np.loadtxt(dataset + ".data.gz")
```

The reference partition consists of $k=3$ clusters:

```{python}
y_true = np.loadtxt(dataset+".labels0.gz", dtype=np.intc)
k = max(y_true)
```

Let us relate it to a few reference partitions.
First, the one generated by the [Genie](https://genieclust.gagolewski.com)
algorithm with the default settings in place:

```{python}
import genieclust
g = genieclust.Genie(n_clusters=k)  # default parameters
y_genie = g.fit_predict(X) + 1
```

Here is the same partition but with the order of clusters changed:

```{python}
o = np.array([1, 3, 2])
(y_genie2 := o[y_genie-1])
```

Second, the k-means method as implemented in
[*scikit-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html):

```{python}
import sklearn.cluster
c = sklearn.cluster.KMeans(n_clusters=k, n_init=1000, random_state=123)
y_km = c.fit_predict(X) + 1
```

Third, the single linkage algorithm (it is a special case of *Genie*):

```{python}
s = genieclust.Genie(gini_threshold=1.0, n_clusters=k)  # single linkage
y_single = s.fit_predict(X) + 1
```


Finally, an imaginary dummy "algorithm" that output a random partition:

```{python}
np.random.seed(909)
y_random = np.random.choice(np.arange(k), len(y_true)) + 1  # sample from 1..k
```

The scatterplots depicting the six partitions are displayed below.
Moreover, the confusion matrices and a few cluster validity measures
are reported.



```{python echo=FALSE}
plt.figure(figsize=(plt.rcParams["figure.figsize"][0], )*2)
np.random.seed(123)  # HIDDEN
```

```{python partition-similarity-example-4,echo=FALSE,results="hide",fig.cap="The reference (ground truth) partition and a few predicted clusterings that we relate to it (wut/x2 dataset); confusion matrices and the values of a few external cluster validity measures are also reported"}

def all_measures(C):
    d = genieclust.compare_partitions.compare_partitions(C)
    return dict(
        AAA=d["aaa"],
        PS=d["psi"],
        SPS=d["spsi"],
        NA=d["nacc"],
        AR=d["ar"],
        R=d["r"],
        NMI=d["nmi"],
        PA=d["nacc"]*(1-1/C.shape[0])+1/C.shape[0],
        FM=d["fm"],
    )

param_txt = dict(x=-30.5, y=19.5, ha="left", va="top", size=10, ma="right")
param_mat = dict(x=-12, y=19, ha="center", va="top",
        family="monospace", ma="left", size=7)

plt.subplot(3, 2, 1)
genieclust.plots.plot_scatter(X, labels=y_true-1, title="Reference", axis="equal")
plt.xlim(-31, 19)
plt.xticks([-10, 10], [])
plt.yticks([0, 10])
plt.legend(labels=["Cluster 1", "Cluster 2", "Cluster 3"])

plt.subplot(3, 2, 3)
genieclust.plots.plot_scatter(X, labels=y_genie-1, title="Genie (0.3)", axis="equal")
plt.xlim(-31, 19)
plt.xticks([-10, 10], [])
plt.yticks([0, 10])
C = genieclust.compare_partitions.confusion_matrix(y_true,y_genie)
v = all_measures(C)
plt.text(**param_txt, s="\n".join(
    ["%s=%.2f"%(l,v[l]) for l in v.keys()]
))
plt.text(**param_mat, s=str(C))

plt.subplot(3, 2, 4)
genieclust.plots.plot_scatter(X, labels=y_genie2-1, title="Genie (0.3) Relabelled",
axis="equal")
plt.xlim(-31, 19)
plt.xticks([-10, 10], [])
plt.yticks([0, 10], [])
C = genieclust.compare_partitions.confusion_matrix(y_true,y_genie2)
v = all_measures(C)
plt.text(**param_txt, s="\n".join(
    ["%s=%.2f"%(l,v[l]) for l in v.keys()]
))
plt.text(**param_mat, s=str(C))

plt.subplot(3, 2, 2)
genieclust.plots.plot_scatter(X, labels=y_km-1, title="KMeans", axis="equal")
plt.xlim(-31, 19)
plt.xticks([-10, 10], [])
plt.yticks([0, 10], [])
C = genieclust.compare_partitions.confusion_matrix(y_true,y_km)
v = all_measures(C)
plt.text(**param_txt, s="\n".join(
    ["%s=%.2f"%(l,v[l]) for l in v.keys()]
))
plt.text(**param_mat, s=str(C))

plt.subplot(3, 2, 5)
genieclust.plots.plot_scatter(X, labels=y_single-1, title="Single Linkage",
axis="equal")
plt.xlim(-31, 19)
plt.xticks([-10, 10])
plt.yticks([0, 10])
C = genieclust.compare_partitions.confusion_matrix(y_true,y_single)
v = all_measures(C)
plt.text(**param_txt, s="\n".join(
    ["%s=%.2f"%(l,v[l]) for l in v.keys()]
))
plt.text(**param_mat, s=str(C))

plt.subplot(3, 2, 6)
genieclust.plots.plot_scatter(X, labels=y_random-1, title="Random", axis="equal")
plt.xlim(-31, 19)
plt.xticks([-10, 10])
plt.yticks([0, 10], [])
C = genieclust.compare_partitions.confusion_matrix(y_true,y_random)
v = all_measures(C)
plt.text(**param_txt, s="\n".join(
    ["%s=%.2f"%(l,v[l]) for l in v.keys()]
))
plt.text(**param_mat, s=str(C))

plt.tight_layout(pad=0)
plt.show()
```

```{python echo=FALSE,results="hide"}
plt.close()  # resets graphical params
```

Actually, both Genie and the k-means method output some quite reasonable
partitions (as we mentioned in {ref}`an earlier section <sec:many-partitions>`,
there might be many equally valid groupings), but here we only wanted to relate
them to a given reference set.




## Why Not Accuracy?

Furthermore, let $\mathbf{C}$ be the confusion matrix,
that is, a matrix with
$k$ rows and $l$ columns,
where, $c_{u,v}$ gives the number point indexes $i$ for which $y_i=u$
and $\hat{y}_v$. In other words, $c_{u,v}$ gives the number of points
from the $u$-th reference cluster that the clustering algorithm
classified as belonging to the $v$-th group.

TODO: citations


```{python}
# (C := genieclust.compare_partitions.confusion_matrix(y_true, y_pred))
```




normalized_confusion_matrix
pivoting... permutes the rows and columns
        so that the sum of the elements of the main diagonal is the largest
        possible



say that this is different than accuracy in classification





genieclust:: bibliography, compare_partitions.pyx

some axioms

Every index except `mi_score` (which computes the mutual
information score) outputs the value of 1.0 if two identical partitions
are given.
Note that partitions are always defined up to a bijection of the set of
possible labels, e.g., (1, 1, 2, 1) and (4, 4, 2, 4)
represent the same 2-partition.

    .. [1]
        Hubert L., Arabie P., Comparing Partitions,
        *Journal of Classification* 2(1), 1985, 193-218.

    .. [2]
        Rendon E., Abundez I., Arizmendi A., Quiroz E.M.,
        Internal versus external cluster validation indexes,
        *International Journal of Computers and Communications* 5(1), 2011,
        27-34.

    .. [3]
        Rezaei M., Franti P., Set matching measures for external cluster validity,
        *IEEE Transactions on Knowledge and Data Mining* 28(8), 2016,
        2173-2186. https://doi.org/10.1109/TKDE.2016.2551240.

    .. [4]
        Vinh N.X., Epps J., Bailey J.,
        Information theoretic measures for clusterings comparison:
        Variants, properties, normalization and correction for chance,
        *Journal of Machine Learning Research* 11, 2010, 2837-2854.

`normalized_accuracy` is defined as
:math:`(\\mathrm{Accuracy}(C_\\sigma)-1/L)/(1-1/L)`,
where :math:`C_\\sigma` is a version of the confusion matrix
for given `x` and `y`, :math:`K \\leq L`, with columns permuted
based on the solution to the Maximal Linear Sum Assignment Problem
(see `normalize_confusion_matrix`).
:math:`\\mathrm{Accuracy}(C_\\sigma)` is sometimes referred to as Purity,
e.g., in [2]_.

`pair_sets_index` gives the Pair Sets Index (PSI)
adjusted for chance [3]_, :math:`K \\leq L`.
Pairing is based on the solution to the Linear Sum Assignment Problem
of a transformed version of the confusion matrix.




`rand_score` gives the Rand score (the "probability" of agreement
between the two partitions) and `adjusted_rand_score` is its version
corrected for chance [1]_ (especially Eqs. (2) and (4) therein):
its expected value is 0.0 for two independent
partitions. Due to the adjustment, the resulting index might also
be negative for some inputs.

Similarly, `fm_score` gives the Fowlkes-Mallows (FM) score
and `adjusted_fm_score` is its adjusted-for-chance version [1]_.

Note that both the (unadjusted) Rand and FM scores are bounded from below
by :math:`1/(K+1)` if :math:`K = L`, hence their adjusted versions
are preferred.

`mi_score`, `adjusted_mi_score` and `normalized_mi_score` are
information-theoretic indices based on mutual information,
see the definition of :math:`\\mathrm{AMI}_\\mathrm{sum}`
and :math:`\\mathrm{NMI}_\\mathrm{sum}` in [4]_.



TODO: citations

The Rand index (1971):

$$
R(\mathbf{C}) = \frac{ {n \choose 2} + 2 \sum_{i,j\in[k]} {c_{i,j}\choose 2}
                  - \sum_{i\in[k]} {c_{i,\cdot}\choose 2}
                  - \sum_{j\in[k]} {c_{\cdot,j}\choose 2}
    }{{n\choose 2}},
$$

under the assumption that ${0\choose 2}={1\choose 2}=0$.


The normalised total number of pairs
$\{\vect{x}_i, \vect{x}_j\}$
with ($u_i=u_j$ and $v_i=v_j$) or ($u_i\neq u_j$ and $v_i\neq v_j$).


\vfill
\bigskip\hrule\bigskip

\tiny

Other (similar) measures: e.g., the Fowlkes--Mallows index,
see Hubert L., Comparing Partitions, {\it Journal of Classification} {\bf 2}, 1985, pp.~193--218 and also M.~Rezaei and P. Fränti. Set matching measures for external cluster validity. \textit{IEEE Transactions on Knowledge and Data Engineering} {\bf 28}(8), 2016, pp.~2173–2186. doi:10.1109/TKDE.2016.2551240.

\nocite{psi}

\end{frame}

\begin{frame}[fragile]\slidetitleA{Comparing Partitions}

Properties:
\begin{itemize}
 \pause\item For every $\mathbf{C}$, $R(\mathbf{C})\ge 0$ and if $\vect{u}=\vect{v}$, then $R(\mathbf{C})=1$;
 \pause\item if $\sigma$ is a permutation of $[k]$
and $\mathbf{C}'$ is such that $c_{i,j}'=c_{i,\sigma(j)}$,
then $R(\mathbf{C})=R(\mathbf{C}')$;
\pause\item if $c_{i,j}=m$ for all $i,j$ and given $k$,
then $R(\mathbf{C}) \stackrel{m\to\infty}{\to} 1-\frac{2(k-1)}{k^2}$.
% \pause\item if $c_{i,j}=m$ for all $j$ and $i=1$ and $c_{i,j}=0$ otherwise,
% then
% $R(\mathbf{C}) \stackrel{m\to\infty}{\to} 1-\frac{k-1}{k}=\frac{1}{k}$.
\end{itemize}
\pause
$\Rightarrow$ The need for a {\color{red2}\bf ``correction for chance''}:

\pause
we'd like the expected index to be equal to $0$ if the
partitions were ``picked at random''.

\medskip\pause
If both $k$-partitions are picked at random, subject to having the original
number of classes and objects in each,
then $
 \mathbb{E}\,  {c_{i,j}\choose 2} =
 \frac{{c_{i,\cdot}\choose 2}{c_{\cdot,j}\choose 2}}{{n\choose 2}}.
$


\bigskip\pause
This yields the \textbf{Adjusted Rand Index} {cite}`comparing_partitions`:

$$
\mathrm{AR}(\mathbf{C})=\tfrac{ \mathrm{R}(\mathbf{C})-\mathbb{E}\,\mathrm{R}(\mathbf{C})}{1-\mathbb{E}\,\mathrm{R}(\mathbf{C})}=
\frac{ \sum_{i,j\in[k]} {c_{i,j}\choose 2}
 - \sum_{i\in[k]} {c_{i,\cdot}\choose 2}
   \sum_{j\in[k]} {c_{\cdot,j}\choose 2} / {n\choose 2}
    }{
     \sum_{i\in[k]} {c_{i,\cdot}\choose 2}/2
   +\sum_{j\in[k]} {c_{\cdot,j}\choose 2}/2
 - \sum_{i\in[k]} {c_{i,\cdot}\choose 2}
   \sum_{j\in[k]} {c_{\cdot,j}\choose 2} / {n\choose 2}
    },
$$

% {\footnotesize\color{gray}
% % see Hubert L., Comparing Partitions, {\it Journal of Classification} {\bf 2}, 1985, p.~193--218.
% }


TODO: ARI

TODO: some axioms


