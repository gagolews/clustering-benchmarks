(sec:noise-points)=
# Noise Points


To make the clustering task more challenging,
some benchmark datasets feature **noise points**
(e.g., outliers or irrelevant points in-between the actual clusters).
They are specially marked in the ground-truth vectors:
we assign them cluster IDs of 0.


## Example

Let us consider the {ref}`other/hdbscan <sec:suite-v1>` dataset
{cite}`hdbscanpkg`, which consists of 2,309 points in $\mathbb{R}^2$.

```{python}
import numpy as np
import pandas as pd
import clustbench
# Accessing <https://github.com/gagolews/clustering-data-v1> directly:
data_url = "https://github.com/gagolews/clustering-data-v1/raw/v1.1.0"
benchmark = clustbench.load_dataset("other", "hdbscan", url=data_url)
X = benchmark.data
y_true = benchmark.labels[0]
```

Here is a summary of the number of points in each reference cluster:

```{python}
pd.Series(y_true).value_counts()
```

There are six clusters (1â€“6) and a special point group with ID=0
that marks some points as noise.

```{python partition-similarity-noise-data,results="hide",fig.cap="An example dataset featuring noise points (light grey whatchamacallits)."}
import genieclust
genieclust.plots.plot_scatter(X, labels=y_true-1, axis="equal", title="y_true")
plt.show()
```

## Discovering Clusters

Suppose we want to evaluate how [Genie](https://genieclust.gagolewski.com)
handles such a noisy dataset.

::::{important}
The algorithm must not be informed about the exact location of the noise points.
After all, it is an unsupervised learning task.
::::


```{python}
k = np.max(y_true)  # the number of clusters to detect
g = genieclust.Genie(n_clusters=k)  # using default parameters
y_pred = g.fit_predict(X) + 1  # +1 makes cluster IDs in 1..k, not 0..(k-1)
```

Below we plot the predicted partition.
Additionally, we draw its version where the
noise point markers are propagated from the ground truth vector
(as a kind of data postprocessing).


```{python partition-similarity-noise-genie,results="hide",fig.cap="Noise points make the life of a clustering algorithm harder."}
plt.subplot(1, 2, 1)
genieclust.plots.plot_scatter(X, labels=y_pred-1, axis="equal", title="y_pred")
plt.subplot(1, 2, 2)
y_pred2 = np.where(y_true > 0, y_pred, 0)  # y_pred, but noise points get ID=0
genieclust.plots.plot_scatter(X, labels=y_pred2-1, axis="equal",
    title="y_pred (noise from y_true)")
plt.show()
```


## Evaluating Similarity

Here is the confusion matrix:

```{python}
genieclust.compare_partitions.confusion_matrix(y_true, y_pred)
```

The first row denotes the "noise cluster".

Genie recreated four of the reference clusters very well (3, 4, 5, 6),
but failed on the first two (it discovered a "combined" cluster instead,
and considered some noise points as a separate set).


::::{important}
When computing external cluster validity measures,
we should omit the noise points from the reference set whatsoever.
After all, most classical algorithms are not equipped with noise point
detectors[^footnoisedetect] and they should not be penalised for this.
::::


Let us compute the normalised clustering accuracy,
ignoring the first row in the confusion matrix:

```{python}
genieclust.compare_partitions.normalized_clustering_accuracy(
    y_true[y_true>0],
    y_pred[y_true>0]
)
```

or, equivalently:

```{python}
clustbench.get_score(y_true, y_pred)  # the NCA metric is used by default
```

The score is somewhere between 4/6 (four clusters discovered correctly)
and 5/6 (one cluster definitely missed).




[^footnoisedetect]: Some algorithms have built-in noise point detectors
    (e.g., [HDBSCAN\*](https://hdbscan.readthedocs.io/en/latest/)
    and [Genie](https://genieclust.gagolewski.com)).
    These can also be evaluated using some of the datasets from our battery,
    but we are not interested in this problem in the current context.
